{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep-Learning-Cell-Seg Pipeline**\n",
    "Welcome! Enclosed is a single lightweight Jupyter notebook for the segmentation of cells and organelles in volume EM datasets.\n",
    "\n",
    "This notebook can be run locally, though your institution likely has a high-performance computing (HPC) cluster that we recommend using. We found it convienient to run the augmentation/post-processing side locally and the neural net side on an HPC. \n",
    "\n",
    "This notebook is broken up into 4 sections: image preparation, network training, network predictions, and post-processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **1: Image Preparation before Training**\n",
    "To start you will need a small stack of raw images and a corresponding stack of labeled features. Our example training data is on OSF.\n",
    "\n",
    "https://osf.io/mpysc/\n",
    "\n",
    "We will also rotate the full raw data stack for multi-axis segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1a:** Training Augmentation\n",
    "We use Albumentations for augmentation, performing elastic deformations, rotations, brightness shifts, contrast shifts, and adding Gaussian noise.\n",
    "\n",
    "https://albumentations.ai/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tifffile import imread, imwrite\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize + Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Small Stack of Raw and Labeled Images\n",
    "input_image_file = r\"C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Training Images\\RawPlatelets.tif\"\n",
    "input_mask_file = r\"C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Training Images\\LabeledPlatelets.tif\"\n",
    "\n",
    "def visualize(image):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "#Note: for optimal resnet18 performance, this should be a multiple of 32\n",
    "patch_size = 512\n",
    "\n",
    "geometry = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=(-0.05, 0.05), scale_limit=(-0.1, 0.1), rotate_limit=(-15, 15), interpolation=1, border_mode=4, value=0, mask_value=0, shift_limit_x=None, shift_limit_y=None, rotate_method=\"largest_box\", always_apply=None, p=0.9),\n",
    "    A.RandomCrop(width=patch_size,height=patch_size,p=1.0),\n",
    "])\n",
    "\n",
    "contrast = A.Compose([\n",
    "    A.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1), hue=(-0.5, 0.5), always_apply=None, p=0.90),\n",
    "])\n",
    "\n",
    "images_stack = imageio.volread(input_image_file)\n",
    "masks_stack = imageio.volread(input_mask_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop for Random Augmentation\n",
    "\n",
    "Takes a paired image and mask in the stack and creates x randomly augmented versions.\n",
    "\n",
    "ImageJ can be very helpful for viewing and managing these .tif stacks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output_as_tiff(augmented_output, output_path):\n",
    "    output_stack = np.stack(augmented_output, axis=0)\n",
    "    imwrite(output_path, output_stack.astype(np.uint8))\n",
    "\n",
    "validation_masks = []\n",
    "validation_images = []\n",
    "\n",
    "training_masks = []\n",
    "training_images = []\n",
    "\n",
    "skip = 1\n",
    "\n",
    "for i in range(len(images_stack) // skip):\n",
    "    for j in range(50):  # To adjust the number of iterations per image\n",
    "        image = images_stack[i * skip]\n",
    "        mask = masks_stack[i * skip]\n",
    "        \n",
    "        # First apply brightness and contrast... we don't need to alter constrast of the mask\n",
    "        image = contrast(image=image)['image']\n",
    "        \n",
    "        # Apply geometric transforms on both image and mask\n",
    "        transformed = geometry(image=image, mask=mask)\n",
    "        transformed_mask = transformed['mask']\n",
    "        transformed_image = transformed['image']\n",
    "\n",
    "        if j % 10 == 0:  # Add every 10th augmentation to validation set\n",
    "            validation_images.append(transformed_image)\n",
    "            validation_masks.append(transformed_mask)\n",
    "        else:\n",
    "            training_images.append(transformed_image)\n",
    "            training_masks.append(transformed_mask)\n",
    "\n",
    "# Save predictions to TIFF files\n",
    "save_output_as_tiff(training_masks, r'C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Augmented Training Images\\Platelet_Masks.tif')\n",
    "save_output_as_tiff(training_images, r'C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Augmented Training Images\\Platelet_Images.tif')\n",
    "save_output_as_tiff(validation_masks, r'C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Augmented Training Images\\Platelet_Masks_validation.tif')\n",
    "save_output_as_tiff(validation_images, r'C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Augmented Training Images\\Platelet_Images_validation.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1b:** Raw Stack Rotation\n",
    "Rotating raw data orientation for multi-axis analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports + Loading Raw Data Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tifffile as tf\n",
    "\n",
    "input_tiff = r\"C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Raw Data\\CellSegPracticeData_ROI1.tif\"\n",
    "output_tiff = r\"C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Raw Data\\CellSegPracticeData_ROI1_yz.tif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotating Data\n",
    "To rotate the xy stack to yz, use (1,2,0).\n",
    "\n",
    "To rotate the xy stack to xz, use (2,0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.TiffFile(input_tiff) as tif:\n",
    "    volume = tif.asarray()  \n",
    "\n",
    "rotated_slices = np.transpose(volume, (1,2,0))\n",
    "\n",
    "# Save to a new .tif as specified above\n",
    "tf.imwrite(output_tiff, rotated_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **2: Neural Network Training**\n",
    "Training using the augmented images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread, imwrite\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization and Training\n",
    "\n",
    "We define a UNet with Resnet18 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle TIFF stacks\n",
    "class TiffDataset(Dataset):\n",
    "    def __init__(self, image_path, mask_path, transform=None):\n",
    "        self.images = imread(image_path)\n",
    "        self.masks = imread(mask_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        \n",
    "        # Convert single-channel image to three channels\n",
    "        image = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Ensure the mask is a single channel\n",
    "        mask = mask[0, :, :].unsqueeze(0)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# U-Net model with ResNet18 backbone\n",
    "class UNetResNet18(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(UNetResNet18, self).__init__()\n",
    "        self.base_model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.encoder1 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.encoder2 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.encoder3 = self.base_layers[5]\n",
    "        self.encoder4 = self.base_layers[6]\n",
    "        self.encoder5 = self.base_layers[7]\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder5 = self._decoder_block(512 + 512, 512)\n",
    "        self.decoder4 = self._decoder_block(512 + 256, 256)\n",
    "        self.decoder3 = self._decoder_block(256 + 128, 128)\n",
    "        self.decoder2 = self._decoder_block(128 + 64, 64)\n",
    "        self.decoder1 = self._decoder_block(64 + 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def _decoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(enc1)\n",
    "        enc3 = self.encoder3(enc2)\n",
    "        enc4 = self.encoder4(enc3)\n",
    "        enc5 = self.encoder5(enc4)\n",
    "\n",
    "        center = self.center(enc5)\n",
    "\n",
    "        dec5 = self.decoder5(center)\n",
    "        dec4 = self.decoder4(torch.cat([dec5, self._crop(enc4, dec5)], 1))\n",
    "        dec3 = self.decoder3(torch.cat([dec4, self._crop(enc3, dec4)], 1))\n",
    "        dec2 = self.decoder2(torch.cat([dec3, self._crop(enc2, dec3)], 1))\n",
    "        dec1 = self.decoder1(dec2)\n",
    "\n",
    "        final = self.final_conv(dec1)\n",
    "        return self._resize(final, x.size()[2:])\n",
    "\n",
    "    def _crop(self, enc, dec):\n",
    "        _, _, H, W = dec.size()\n",
    "        enc = transforms.CenterCrop([H, W])(enc)\n",
    "        return enc\n",
    "\n",
    "    def _resize(self, input, size):\n",
    "        return nn.functional.interpolate(input, size=size, mode='bilinear', align_corners=True)\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((512 , 512))  # Again, ensure the size is a multiple of 32\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Availability\n",
    "Ensure you are utilizing a GPU, confirmed by 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = UNetResNet18(n_classes=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = TiffDataset(\n",
    "    '/gpfs/gsfs12/users/baenencm/Platelet_Images.tif', \n",
    "    '/gpfs/gsfs12/users/baenencm/Platelet_Masks.tif', \n",
    "    transform=transform\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "validation_data = TiffDataset(\n",
    "    '/gpfs/gsfs12/users/baenencm/Platelet_Images_validation.tif', \n",
    "    '/gpfs/gsfs12/users/baenencm/Platelet_Masks_validation.tif', \n",
    "    transform=transform\n",
    ")\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = UNetResNet18(n_classes=1).to(device)  # Move model to GPU if available\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Choose # of training epochs\n",
    "num_epochs = 7\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images, masks = images.to(device), masks.to(device)  # Move data to GPU if available\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "\n",
    "    for val_images, val_masks in tqdm(validation_dataloader):\n",
    "        val_images, val_masks = val_images.to(device), val_masks.to(device)  # Move data to GPU if available\n",
    "        val_outputs = model(val_images)\n",
    "        loss = criterion(val_outputs, val_masks)\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss/len(dataloader)}')\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss/len(validation_dataloader)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Network Output Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/gpfs/gsfs12/users/baenencm/Practice_Platelet_Membranes.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(30): # set to however many original training images you labeled\n",
    "        image, mask = validation_data[i*10]\n",
    "        image = image.unsqueeze(0).to(device)  # move data to GPU if available\n",
    "        output = model(image)\n",
    "        output = torch.sigmoid(output).cpu().squeeze().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axes[0].imshow(image.cpu().squeeze().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "        axes[0].set_title('Input Image')\n",
    "        axes[1].imshow(mask.cpu().squeeze().numpy(), cmap='gray')\n",
    "        axes[1].set_title('Ground Truth')\n",
    "        axes[2].imshow(output*100000, cmap='gray')\n",
    "        axes[2].set_title('Prediction')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **3: Network Predictions**\n",
    "### Function to segment small patches of the image, then stitch back together\n",
    "We recommend to use the same patch size as the image size the network was trained on in the previous section. We found that padding = patch_size/2 and stride = (patch_size/2)-remove_edge worked well for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image, model, device, patch_size=512, stride=244):\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Ensure image is NumPy array\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().numpy()\n",
    "        \n",
    "    # Ensure image has 3 channels by stacking\n",
    "    if image.ndim == 2:\n",
    "        image = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "    # Initialize   empty array for final mask\n",
    "    padding = 256\n",
    "    remove_edge = 12\n",
    "    \n",
    "    # Pad the image\n",
    "    image = np.pad(image, ((padding, padding*2), (padding, padding*2), (0, 0)), mode='constant',constant_values=100)\n",
    "    #plt.imshow(image)\n",
    "\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    _, _, H, W = image_tensor.shape\n",
    "\n",
    "    mask = np.zeros((H, W))\n",
    "\n",
    "    # Divide image into overlapping patches\n",
    "    for i in range(0, H - patch_size + 1, stride):\n",
    "        for j in range(0, W - patch_size + 1, stride):\n",
    "            patch = image_tensor[:, :, i:i + patch_size, j:j + patch_size]\n",
    "            with torch.no_grad():\n",
    "                output = model(patch)\n",
    "                output_mask = torch.sigmoid(output).cpu().squeeze().numpy()\n",
    "                mask[i+remove_edge:i + patch_size-remove_edge, j+remove_edge:j + patch_size-remove_edge] += output_mask[remove_edge:-remove_edge,remove_edge:-remove_edge]\n",
    "\n",
    "    return mask[padding:H-2*padding,padding: W-2*padding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Trained Network\n",
    "\n",
    "If doing multi-plane analysis, run the xy, yz, and xz stacks here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetResNet18(n_classes=1).to(device)  # Move model to GPU if available\n",
    "model.load_state_dict(torch.load('/gpfs/gsfs12/users/baenencm/Practice_Platelet_Membranes.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "stack = imread(\"/gpfs/gsfs12/users/baenencm/CellSegPracticeData_ROI1.tif\")\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(0,len(stack)), desc=\"Processing Images\", unit=\"image\"):\n",
    "    images = stack[i]\n",
    "    outputs = segment_image(images, model, device)\n",
    "    predictions.append(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(predictions),50): \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    axes[0].imshow(stack[i], cmap='gray')\n",
    "    axes[0].set_title(f'Input: Slice {i}')\n",
    "    cax = axes[1].imshow(predictions[i], cmap='magma')\n",
    "    axes[1].set_title('Network Prediction')\n",
    "    cbar = fig.colorbar(cax, ax=axes[1])\n",
    "    cbar.set_label('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to save predictions as TIFF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_as_tiff(predictions, output_path):\n",
    "    predictions = [np.array(p, dtype=np.float16) for p in tqdm(predictions, desc=\"Processing predictions\")]\n",
    "    predictions_stack = np.stack(predictions, axis=0)\n",
    "    imwrite(output_path, predictions_stack)\n",
    "\n",
    "save_predictions_as_tiff(predictions, '/gpfs/gsfs12/users/baenencm/Practice_Platelet_Membranes_xy.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **4: Post-Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotating Data back to xy.\n",
    "To rotate the yz prediction stack back to xy, use (2,0,1).\n",
    "\n",
    "To rotate the xz prediction stack back to xy, use (1,2,0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tiff = r\"C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Raw Data\\Practice_Platelet_Membranes_yz.tif\"\n",
    "output_tiff = r\"C:\\Users\\baenencm\\Desktop\\Platelet Example Dataset\\Raw Data\\Practice_Platelet_Membranes_yz_rotated_back.tif\"\n",
    "\n",
    "with tf.TiffFile(input_tiff) as tif:\n",
    "    volume = tif.asarray()  \n",
    "\n",
    "rotated_slices = np.transpose(volume, (1,2,0))\n",
    "\n",
    "# Save to a new .tif as specified above\n",
    "tf.imwrite(output_tiff, rotated_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From this point, we used Amira to merge the three masks and perform the rest of the analysis. More information on this process can be found in the full paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
